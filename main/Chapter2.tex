\chapter{ORFEO infrastructure: distributed storage and identity services}\label{chap:infra}

\section{ORFEO datacenter and distributed storage}

This section introduces ORFEO’s computing and storage infrastructure, which
serves as the backbone for research workflows at Area Science Park. We begin
with the datacenter itself and its physical storage, then describe the Ceph
filesystem, and finally examine the RADOS Gateway interface.

\subsection{ORFEO datacenter: infrastructure and physical data storage}

ORFEO is a cutting-edge data center managed by the Laboratory of Data 
Engineering (LADE) at Area Science Park. It is housed in a specialized container 
and provides advanced computing and data services for scientific 
research \parencite{ORFEO_Docs_Home,LADEPage}. 

\medskip

In terms of hardware, ORFEO offers:
\begin{itemize}
	\item high-performance computing nodes (including GPU-equipped servers 
	and large-memory nodes up to 1.5\,TB RAM), 
	\item interconnected with high-speed networks (\texttt{InfiniBand} 100\,Gb/s) \parencite{ORFEO_Docs_Computational}.
\end{itemize}

To support data-intensive workloads, ORFEO’s storage infrastructure delivers 
over 5\,PB of raw storage capacity, implemented as a parallel distributed 
filesystem. This storage is served to users via a \texttt{Ceph} cluster, which 
combines:
\begin{itemize}
	\item high-speed flash storage tiers (\texttt{NVMe} and \texttt{SSD} drives), 
	\item standard capacity tiers (\texttt{HDD} drives) \parencite{ORFEO_Docs_Storage,orfeo-changelog-2024}.
\end{itemize}

A long-term archival storage partition of more than 3\,PB is available for data 
retention, and a tape library is also present for backups and cold 
storage \parencite{ORFEO_Docs_Storage,orfeo-changelog-2024}. 

\medskip

In summary, ORFEO physically stores data across a large \texttt{Ceph} 
distributed storage system augmented by dedicated archival appliances, ensuring 
both high-performance access and reliable long-term preservation of research 
data.

\medskip

From a physical perspective, the \texttt{Ceph} storage cluster underlying ORFEO 
consists of numerous storage nodes (servers with many disks) acting together. 
ORFEO’s shared filesystem is hosted on a \texttt{Ceph} cluster of 23 storage 
nodes, providing roughly 4.8\,PiB of raw space across the 
cluster \parencite{ORFEO_Docs_Storage,orfeo-changelog-2024}. 

The \texttt{Ceph} cluster is configured with different tiers to meet varying 
performance needs:
\begin{itemize}
	\item A “fast” pool of four \texttt{Ceph} nodes with \texttt{SSD}s (about 
	279\,TiB raw capacity) for high-speed scratch storage.
	\item 19 \texttt{Ceph} nodes with \texttt{HDD}s providing a larger capacity 
	pool for standard storage (home directories and bulk scratch 
	data) \parencite{ORFEO_Docs_Storage}.
\end{itemize}

In practice, user data is striped and replicated across many disks and servers: 
\texttt{Ceph}’s distributed design writes pieces of each file (as objects) to 
multiple nodes, protecting against hardware failures. Thanks to \texttt{Ceph}’s 
redundancy (via replication or erasure coding), the data center can tolerate 
disk or node failures without losing data \parencite{Ceph_RADOS,Ceph_Pools,Ceph_EC}. 

The result is a highly reliable storage backbone: users see a single unified 
filesystem, but underneath, their data blocks are physically spread across many 
drives and machines in the ORFEO cluster.

\medskip

It’s worth noting how different research groups utilize ORFEO’s storage. All 
LADE-managed projects and users have their data on the \texttt{Ceph}-based 
filesystem by default. Each user gets:
\begin{itemize}
	\item a home directory (with quotas such as 200\,GB and 1\,million files 
	enforced) on a \texttt{CephFS} volume backed by \texttt{SSD} storage 
	(with triple replication for safety) \parencite{ORFEO_Docs_Storage}.
	\item access to a shared scratch space on \texttt{Ceph}—a large 
	high-capacity area on \texttt{HDD}-based storage for active analyses. This 
	scratch is configured with an erasure-coded scheme (6+2 encoding, meaning 
	data is split into 6 chunks plus 2 parity chunks) to maximize usable space 
	while still tolerating failures \parencite{Ceph_EC,Ceph_RADOS}.
	\item a per-user fast scratch area on \texttt{SSD}s (also replicated 3$\times$) 
	for I/O-intensive tasks requiring faster throughput \parencite{Ceph_Pools}.
\end{itemize}

In contrast, the LAME laboratory (Microscopy lab) historically has stored its 
data on local storage systems separate from ORFEO’s \texttt{Ceph} cluster. This 
means LAME’s data hasn’t benefited from ORFEO’s centralized distributed storage. 

\medskip

A key motivation for our work is to enable transferring and integrating LAME’s 
locally stored data into ORFEO, so that microscopy data can reside on the 
\texttt{Ceph}-based infrastructure alongside the genomics and other datasets 
managed by LADE. (The genomics lab, LAGE, is already set up to use ORFEO’s 
storage, so we focus here on LADE and LAME integration.) 

By moving LAME’s data into ORFEO, researchers will gain the advantages of 
\texttt{Ceph}’s reliability, scalability, and unified access for all their 
scientific data.


\subsection{Ceph filesystem in ORFEO (\texttt{CephFS})}

\texttt{Ceph} is the primary storage technology underpinning ORFEO’s 
filesystem \parencite{ORFEO_Docs_Storage}. \texttt{CephFS} (\texttt{Ceph} 
Filesystem) is a \texttt{POSIX}-compliant distributed file system built on 
\texttt{Ceph}’s object store (\texttt{RADOS}) \parencite{CephFS}. 

In simpler terms, \texttt{CephFS} allows all compute nodes in the cluster to 
see a shared directory tree (“/orfeo/\ldots”) where user files and project data 
reside, just like a network file system, but with no single server bottleneck. 
Instead of storing files on a single disk or server, \texttt{CephFS} distributes 
file data in objects across the cluster’s OSDs (Object Storage Daemons)—essentially 
across many disks on many nodes—and a cluster of Metadata Servers (MDS) 
coordinates the file system namespace and directory 
operations \parencite{CephFS,Ceph_RADOS,RedHat_CephFS}. 

This architecture provides high throughput and availability: clients (compute 
nodes or user login nodes) directly read/write data to the storage nodes in 
parallel, while metadata (filenames, directories, permissions) is handled by 
dedicated MDS servers for efficient lookup \parencite{CephFS,RedHat_CephFS}. As a 
result, \texttt{CephFS} can scale out linearly with the size of the cluster, 
making it ideal for HPC scenarios like ORFEO where numerous users and jobs 
concurrently access shared datasets \parencite{CephFS}.

\medskip

One of the strengths of \texttt{CephFS} in ORFEO is its built-in redundancy and 
data management policies. \texttt{Ceph} transparently manages data replication 
and/or erasure coding across the cluster, so users do not have to worry about 
manually copying data to protect against failures \parencite{Ceph_Pools,Ceph_EC}. 

For instance, as noted above, some \texttt{Ceph} pools on ORFEO keep three 
replicas of each data block on different storage nodes (triple mirroring) for 
important or hot data \parencite{Ceph_Pools}. Other pools use an erasure-coded 
scheme (6+2), which breaks each file into 6 data chunks and 2 parity chunks 
distributed to different nodes; any 6 of the 8 chunks are sufficient to recover 
the file, giving fault tolerance with only $\sim$1.33$\times$ storage overhead 
instead of 3$\times$ \parencite{Ceph_EC,Ceph_RADOS}. 

\texttt{Ceph} automatically handles re-balancing and healing: if a disk or node 
fails, the system detects the loss and recreates missing chunks or copies on 
healthy devices in the cluster. This self-healing design, along with capacity 
scaling by simply adding more nodes or disks, makes \texttt{Ceph} a very robust 
solution for a growing data center \parencite{Ceph_RADOS}. Moreover, 
\texttt{CephFS} supports features like quotas on directories (e.g.\ enforcing 
per-user storage limits) and snapshots, which are useful in a multi-user 
research environment \parencite{ORFEO_Docs_Storage}. 

All these capabilities mean that ORFEO’s \texttt{Ceph}-based filesystem can 
simultaneously serve as home directories, scratch spaces, and project archives 
for many users without performance bottlenecks or single points of failure.

\medskip

In ORFEO’s configuration, the \texttt{CephFS} is mounted under the 
\texttt{/orfeo} directory on all cluster machines, providing a unified space for 
data. As outlined earlier, this space is organized into subfolders like 
\texttt{home}, \texttt{scratch}, \texttt{fast}, etc., each possibly backed by 
different storage tiers \parencite{ORFEO_Docs_Storage}. 

Users’ home directories (in \texttt{/orfeo/cephfs/home}) are intended for 
personal files and configurations and have a fixed quota (e.g.\ 
200\,GiB) \parencite{ORFEO_Docs_Storage}. The scratch areas (in 
\texttt{/orfeo/cephfs/scratch}) are shared working spaces for active 
computations, visible on all nodes and divided into project-specific subfolders; 
these reside on high-capacity 10k\,RPM \texttt{HDD}s, and while they have no 
strict quota, older files may be purged after a minimum retention period (e.g.\ 
30 days) to free space \parencite{ORFEO_Docs_Storage}. 

The fast area (accessible via \texttt{/orfeo/cephfs/fast}) is another shared 
scratch intended for workloads needing faster I/O, stored on \texttt{SSD}s; 
similarly un-quotaed, it’s managed with a policy to clear old data as 
needed \parencite{ORFEO_Docs_Storage}. 

Finally, for archival storage, ORFEO provides a Long-Term Storage (LTS) 
separate from \texttt{CephFS}: this is implemented via a Dell PowerVault 
\texttt{SAN} (Network Attached Storage using \texttt{NFSv4}) offering 
$\sim$2.8\,PiB raw space, allocated in volumes per group/project for long-term 
data retention \parencite{ORFEO_Docs_Storage}.

\medskip

In summary, \texttt{CephFS} handles the high-performance and working-storage 
tiers (home, scratch, fast) for LADE users on ORFEO, while a traditional 
\texttt{NAS} and tape handle archival storage. The key point is that every LADE 
user’s active data partition is on \texttt{Ceph}—giving them the benefits of 
distributed storage—whereas LAME users currently keep data on local storage 
outside this system. 

Bridging that gap is a goal of integrating LAME with ORFEO, allowing microscopy 
data to be moved into \texttt{CephFS} where it can be easily shared and 
processed in the HPC environment.


\subsection{Ceph RADOS Gateway and the S3 API}

In addition to providing file-system access through \texttt{CephFS}, the 
\texttt{Ceph} storage platform also supports an object storage interface called 
the RADOS Gateway (RGW) \parencite{Ceph_RGW_Overview}.

\medskip

To unpack this: \texttt{Ceph} is built on top of a distributed storage system 
called Reliable Autonomic Distributed Object Store (RADOS), which handles the 
actual storage of data across many servers and disks. The \texttt{RADOS Gateway} 
is a service (a daemon called \texttt{radosgw}) that runs as an HTTP server. 
Instead of presenting storage as files and directories, the gateway exposes a 
web-based API—in other words, a network endpoint that applications can talk to 
using standard internet protocols \parencite{Ceph_RGW_Overview,Ceph_RGW_S3}.

\medskip

The most important feature of the RADOS Gateway is that its interface is 
designed to be compatible with the Amazon Simple Storage Service (Amazon S3) 
API \parencite{Ceph_RGW_S3}. Amazon S3 is one of the earliest and most widely 
adopted cloud storage services, introduced by Amazon Web Services in 2006. It 
popularized the concept of “object storage,” where data is managed as discrete 
units called objects rather than as files in a traditional hierarchical file 
system. Each object consists of the raw data (for example, the contents of a 
file) along with associated metadata (such as its size, creation date, or custom 
tags). Objects are organized into logical containers called buckets. Access to 
objects is performed through API calls sent over HTTP or HTTPS, such as “PUT” 
to upload a file, “GET” to download it, or “DELETE” to remove it.

\medskip

By imitating this interface, the Ceph RADOS Gateway makes it possible for 
applications and users to interact with Ceph’s storage as if they were 
interacting with Amazon S3 itself \parencite{Ceph_RGW_S3,Ceph_RGW_Overview}. This 
compatibility is powerful: it means that existing tools, libraries, and 
workflows that already support Amazon S3 can be used directly with a Ceph 
cluster, without modification. For example, common command-line utilities, 
backup systems, scientific data management pipelines, or even web applications 
can all speak the S3 protocol and therefore work seamlessly with Ceph when the 
RADOS Gateway is deployed \parencite{Ceph_RGW_S3}.

\medskip

Internally, the RADOS Gateway does not bypass Ceph’s design. All objects that 
arrive through the S3 interface are still stored within the underlying RADOS 
cluster, just like the data written via CephFS \parencite{Ceph_RGW_Overview}. This 
ensures the same level of redundancy, fault tolerance, and scalability. However, 
the object storage model is different from a file system model: instead of 
directories and file paths, users think in terms of buckets and object keys. The 
gateway also maintains its own system of user accounts, credentials, and access 
control rules, separate from Linux accounts on the cluster, which allows 
external collaborators to interact with the storage securely without being given 
direct access to the cluster’s internal 
filesystem \parencite{Ceph_RGW_Overview,Ceph_RGW_S3}.

\medskip

The RADOS Gateway supports many features of Amazon S3, such as bucket creation, 
object versioning, fine-grained access policies, and metadata 
management \parencite{Ceph_RGW_S3}. Moreover, Ceph’s unified design means that 
data stored through one interface can, in some cases, be accessed through 
another: for example, data written via the S3 protocol could also be read using 
OpenStack Swift (another popular object storage 
protocol) \parencite{Ceph_RGW_Overview}. This interoperability highlights Ceph’s 
flexibility as a storage backend.

\medskip

From the perspective of ORFEO, deploying a Ceph RADOS Gateway would allow 
research groups and external partners to upload, download, and manage their data 
through the familiar S3 protocol over standard web connections. For instance, a 
laboratory could transfer large microscopy datasets to ORFEO simply by 
targeting the S3 endpoint with existing tools, rather than having to mount a 
network file system or rely on manual file 
transfers \parencite{Ceph_RGW_S3}. This can greatly simplify data sharing and 
integration, especially in collaborative environments where not all users have 
direct logins to the HPC infrastructure.

\medskip

Looking ahead, this S3 compatibility also creates a natural integration point 
with higher-level software. Many modern web frameworks and data platforms 
(including the Django web framework, which we will consider later) offer 
built-in or plug-in support for S3 storage backends. This means that a 
Django-based web portal running on ORFEO could treat the Ceph cluster’s object 
storage just like Amazon S3, using standard libraries to upload, retrieve, and 
manage user files \parencite{Ceph_RGW_S3,Ceph_RGW_Overview}.

\medskip

In summary, the Ceph RADOS Gateway transforms the Ceph cluster into a private 
cloud storage service by exposing an S3-compatible object storage 
API \parencite{Ceph_RGW_S3,Ceph_RGW_Overview}. While CephFS provides traditional 
file-based access within the cluster, RGW makes the same reliable storage 
accessible externally through web protocols, enabling secure and scriptable data 
exchange with collaborators, laboratories, and applications. This dual 
approach—file system access for HPC workloads and object storage for external 
and web-based workflows—broadens ORFEO’s utility as a central data platform.


\section{Identity and access management (IAM): FreeIPA with Authentik}

Modern computing infrastructures typically provide many different services 
(web portals, storage systems, analysis tools), but users should not be forced 
to maintain separate usernames and passwords for each. Instead, organizations 
deploy an \textbf{Identity and Access Management (IAM)} system that centralizes 
accounts and authentication. This ensures that users have \emph{one set of 
	credentials} to access multiple services, while administrators retain a single 
point of control for enabling, disabling, or updating accounts.

\medskip

In ORFEO, IAM follows a \emph{directory\,$\rightarrow$\,identity provider} 
pattern. The authoritative directory is provided by 
\textbf{FreeIPA}\footnote{%
	FreeIPA is an open-source identity management platform that combines two 
	major components: (i) an \emph{LDAP} directory (Lightweight Directory Access 
	Protocol) for storing structured identity data such as users and groups, and 
	(ii) a \emph{Kerberos} realm, a ticket-based authentication protocol 
	originally developed at MIT for secure single sign-on in distributed systems. 
	FreeIPA is also the upstream community version of Red Hat Identity Manager.}, 
while the user-facing single sign-on layer is provided by 
\textbf{Authentik}\footnote{%
	Authentik is an open-source \emph{identity provider (IdP)}, meaning it is the 
	system that users directly log into. It supports modern web authentication 
	standards such as OAuth2, OIDC (OpenID Connect), and SAML (Security Assertion 
	Markup Language). Comparable commercial systems include Okta and Auth0; a 
	popular open-source alternative is Keycloak.}. 

In this arrangement, FreeIPA acts as the \emph{source of truth} for identities, 
while Authentik synchronizes with FreeIPA via LDAP to provide a standards-based 
login and token service for web applications 
\parencite{Authentik_Docs_LDAP,FreeIPA_Overview}. The two roles can be 
summarized as:

\begin{itemize}
	\item \emph{FreeIPA} → manages user and group accounts, and provides the 
	underlying directory.  
	\item \emph{Authentik} → synchronizes with FreeIPA and acts as an SSO gateway, 
	issuing tokens that client applications can validate.  
\end{itemize}

\subsection*{Token flow and claims}
When a user attempts to access a protected application (for example, a Django 
portal), the application redirects the user to Authentik for sign-in. 
Authentik validates the credentials against FreeIPA’s directory, and on success 
issues an \textbf{ID Token}\footnote{%
	An ID Token in OIDC is a \emph{JSON Web Token (JWT)}, i.e.\ a compact, signed 
	JSON object. A JWT can carry information (``claims'') such as username, 
	email, or group membership, and can be cryptographically verified by the 
	application without contacting the IdP again.}. The application trusts this 
token, as configured in its OIDC client settings, and establishes a session for 
the user. Importantly, the application itself never handles raw passwords: 
authentication is fully delegated to Authentik and FreeIPA 
\parencite{Authentik_Docs_OIDC,OpenIDConnectCore}. This mechanism enables 
\textbf{single sign-on (SSO)} across multiple services with a single set of 
credentials.

\subsection*{Group-based authorization}
Beyond authentication, this setup supports centralized authorization. Because 
Authentik imports groups from FreeIPA during synchronization, it can embed group 
information as \textbf{claims}\footnote{%
	In OIDC, a claim is a key--value pair inside a token that provides identity 
	attributes, such as \texttt{email=alice@example.org} or 
	\texttt{groups=[researchers,admins]}.} in the tokens it issues. Applications 
can then map these group claims to local roles or permissions. For example, all 
members of a FreeIPA group \texttt{app-admins} could automatically become 
administrators in a Django web application. Administrators only need to manage 
membership once in FreeIPA, and the changes propagate consistently to all 
relying applications \parencite{Authentik_Docs_Claims,FreeIPA_Docs}. 

\medskip

In summary, ORFEO’s IAM solution combines FreeIPA for directory and group 
management with Authentik for OIDC-based single sign-on. This separation of 
responsibilities improves both security (applications never store passwords) and 
usability (users log in once, then access multiple services). The same pattern 
is reproduced and validated in the VirtualOrfeo environment, as described in 
Chapter~\ref{chap:virtualorfeo-deployment}.
