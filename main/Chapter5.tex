\chapter{Deep Dive into the Application}\label{chap:deep-dive-app}

This chapter presents an in-depth, implementation-oriented analysis of the Django application for experimental data management. The system follows a layered architecture that separates domain rules, storage abstractions, background execution, and presentation. It integrates with an external identity provider (Authentik via OIDC), persists raw and derived artefacts to an S3-compatible object store (Ceph RGW/MinIO), and uses Redis Queue (RQ) for asynchronous processing. The following sections detail the model, data flows, metadata pipeline, NeXus construction path, storage gateway, background jobs, API/UI surfaces, the in-app security model, and the performance/scalability envelope.

\section{Domain model and data flow}\label{sec:domain-dataflow}

\paragraph{Entities.}
The core data model comprises four first-class entities:
\begin{itemize}
	\item \textbf{Project} (e.g.\ RIANA, NFFA\_DI): top-level namespace; immutable \texttt{name} with a derived \texttt{slug} (normalized via a local \texttt{slugify} helper).
	
	\item \textbf{Proposal}: belongs to a \textbf{Project}, uniquely identified by the pair (\texttt{project}, \texttt{number}), with fields \texttt{pi\_name}, \texttt{date}, \texttt{description}, and \texttt{created\_by}. Each Proposal owns one S3 bucket name (\texttt{bucket}, unique and non-editable) that is deterministically generated on first save as \texttt{<prefix>+project.slug-number-sha1[:8]}.
	
	\item \textbf{Sample}: belongs to a \textbf{Proposal}; unique on (\texttt{proposal}, \texttt{slug}), where \texttt{slug} derives from \texttt{name}. Optional descriptive fields (\texttt{identifier}, \texttt{preparation\_date}, \texttt{atom\_types}, \texttt{physical\_form}) are accepted and written into README artefacts.
	
	\item \textbf{Experiment}: belongs to a \textbf{Sample}; unique on (\texttt{sample}, \texttt{slug}), with optional \texttt{description} and start-time metadata.
\end{itemize}

Uniqueness and referential integrity constraints are enforced at the ORM level (model \texttt{Meta} constraints and \texttt{unique\_together}). The domain orchestration functions in \texttt{domain/commands.py} (\texttt{create\_proposal}, \texttt{create\_sample}, \texttt{create\_experiment}) encapsulate transactional creation of rows, idempotent bucket provisioning, and side-effects such as README materialization.

\paragraph{Lifecycle and flow.}
The normative lifecycle is:

\begin{enumerate}
	\item \textbf{Creation via forms or API.} HTMX modals (\texttt{view\_modals.py}) call domain commands to create entities. On success, the modal returns \texttt{204 No Content} with an HTMX trigger to refresh panes; on validation errors, the modal re-renders with a single aggregated error block (no 4xx).
	
	\item \textbf{README materialization.} Immediately after each create, a README is written to object storage through the storage gateway: \texttt{<proposal\_prefix>/README.txt}, \texttt{<sample\_prefix>/README.txt}, and \texttt{<experiment\_prefix>/README.txt}. Bodies are produced by \texttt{services/readme.py}. This approach makes per-entity metadata co-resident with data.
	
	\item \textbf{Measurement registration and raw upload.} A user selects the target (\texttt{Proposal}, \texttt{Sample}, \texttt{Experiment}) and uploads raw files directly from the browser to S3 via presigned PUT (generated by \texttt{/s3\_presign}). The browser submits the uploaded S3 keys and minimal metadata (\texttt{pair}, \texttt{user\_name}, \texttt{meas\_desc}) to \texttt{register\_measurement()}.
	
	\item \textbf{Measurement README and background jobs.} \texttt{register\_measurement()} writes a measurement-level README next to the files and enqueues background jobs (checksum computation for every uploaded key; NeXus conversion for eligible TIFF keys). Buckets for both RAW and mirroring use cases are ensured to exist.
	
	\item \textbf{Mirroring to derived buckets.} The app computes a mirrored bucket name via \texttt{mirror\_bucket\_for()} and ensures its creation so that derived artefacts (e.g.\ NeXus) can be published in a separate, discoverable namespace. The \texttt{/ofed/} view shows this mirrored perspective.
\end{enumerate}

\paragraph{Invariants and failure handling.}
Domain commands use early reads for friendly validation and wrap writes in atomic transactions; collisions race-safe fall back to uniqueness rechecks. Storage-side provisioning (\texttt{create\_bucket}) is idempotent. Background checksum updates use \texttt{update\_or\_create} to be idempotent under retries.

\section{Metadata path}\label{sec:metadata-path}

\paragraph{TIFF ingestion.}
Raw microscopy outputs are commonly TIFFs. Upon upload, only keys are persisted synchronously; content processing is deferred to workers. TIFF reading is performed in builder services (see \S\ref{sec:nexus-construction}). For streaming paths, the app uses S3’s \texttt{StreamingBody} to avoid memory blow-ups; chunk size defaults (8–16,MiB) are set in checksum and ZIP streaming code.

\paragraph{Instrument metadata extraction and mapping.}
To decouple instrument idiosyncrasies from NeXus semantics, JSON mapping files under \texttt{experiment\_manager/data/} (e.g.\ \texttt{ED\_mapping.json}, \texttt{TVIPS\_mapping.json}) define per-pair field correspondences. The selected pair (instrument\_detector) is carried from the UI (\texttt{PAIRS} in \texttt{views.py}) through the \texttt{register\_measurement()} call and ultimately into builder orchestration.

\paragraph{Translation to NXem concepts.}
The NXem application definition (NeXus for electron microscopy) expects a standardized hierarchy (entry/instrument/detector/sample, with semantic fields for pixel sizes, accelerating voltage, camera length, dwell time, etc.). The mapping layer translates extracted TIFF tags and accompanying metadata into this semantic schema. Practically:

\begin{itemize}
	\item \emph{Source fields}: TIFF tags (ImageDescription, XResolution/YResolution, Software), sidecar text from Measurement README, and form inputs (operator, description).
	\item \emph{Mapping}: JSON selects source keys (including regex or path expressions in \texttt{ImageDescription}) and yields destination NeXus paths (e.g.\ \texttt{/entry/instrument/detector/exposure\_time}).
	\item \emph{Normalization}: unit coercion (s to ms, nm to m), type casting, and defaults when missing.
\end{itemize}

This arrangement localizes pair-specific complexity in data files, keeping the builder logic generic and testable.

\section{NeXus construction}\label{sec:nexus-construction}

\paragraph{Builders and orchestration modules.}
The construction path splits into:
\begin{itemize}
	\item \texttt{services/nexus\_builders.py}: low-level writers for \texttt{.nxs} that accept already mapped/normalized metadata and the raw TIFF stream; responsible for producing a valid HDF5/NXem tree.
	\item \texttt{services/nexus.py}: orchestration that decides target locations (RAW vs.\ mirrored buckets), resolves output keys, and wraps storage interactions; provides \texttt{build\_and\_upload\_nexus(...)} and helper utilities like \texttt{mirror\_bucket\_for(...)}.
\end{itemize}

\paragraph{Idempotency and deduplication.}
Workers enqueue with a stable \texttt{job\_id} derived from (\texttt{pair}, \texttt{bucket}, \texttt{key}); a sanitized slug avoids Redis key pathologies. Before enqueueing, the queue is probed (\texttt{fetch\_job}) to skip duplicates. On the storage side, builders deterministically produce a destination key (e.g.\ under an \texttt{nx/} or \texttt{processed/} prefix) so re-runs overwrite the same object (or are guarded by ETag checks) rather than proliferating variants.

\paragraph{Validation.}
Builder steps include:
\begin{enumerate}
	\item Schema completeness checks after mapping (required NXem fields present, with sensible defaults otherwise).
	\item TIFF data sanity (dimensions, dtype) prior to embedding or linking.
	\item Post-write verification: open the produced HDF5 and probe a minimal set of nodes/attributes to confirm structural validity.
\end{enumerate}

\paragraph{File structure and checks.}
Resulting \texttt{.nxs} files follow:
\begin{itemize}
	\item \texttt{/entry} with \texttt{NXentry} class and experiment description, timestamps, and operator.
	\item \texttt{/entry/instrument} subtree with \texttt{NXinstrument}, including \texttt{NXdetector} attributes for camera metadata.
	\item \texttt{/entry/sample} for sample descriptors captured from domain entities and README.
	\item \texttt{/entry/data} linking datasets to detector frames; axes and units defined for downstream tools.
\end{itemize}
A SHA-256 digest is computed for original raw files (see \S\ref{sec:background-tasks}); optional digests for \texttt{.nxs} outputs can be added to ensure end-to-end integrity.

\section{Storage gateway}\label{sec:storage-gateway}

\paragraph{Abstraction and contracts.}
The app standardizes storage via a \texttt{StorageGateway} Protocol (\texttt{storage.py}) with methods for bucket lifecycle, object IO, and presigning (\texttt{presign\_get}, \texttt{presign\_put}). The default \texttt{\_BotoStorage} delegates to \texttt{services/s3.py}. A test-time in-memory gateway (\texttt{storage\_fake.py}) implements the same interface to decouple unit tests from S3.

\paragraph{Key naming (\texttt{pathing.py}).}
Canonical prefixes encode the domain hierarchy:
\begin{center}
	\texttt{<proposal\_prefix>(proposal)/README.txt}\
	\texttt{<sample\_prefix>(sample)/README.txt}\
	\texttt{<experiment\_prefix>(experiment)/README.txt}
\end{center}
Measurement READMEs are placed adjacent to uploaded files (folder-local \texttt{README.txt}) to improve locality in browsing and auditability.

\paragraph{Bucket strategy.}
Per-proposal buckets isolate tenancy and simplify lifecycle management. Names are deterministic and collision-resistant (8-hex SHA-1 suffix). \texttt{create\_bucket} is idempotent and also applies bucket-level CORS rules. Mirrored buckets (e.g.\ OFED) derive from the RAW name (\texttt{mirror\_bucket\_for}) and are ensured ahead of publishing derived outputs.

\paragraph{Upload patterns (presigned PUT).}
The browser obtains a presigned PUT URL from \texttt{/s3\_presign} by providing (\texttt{bucket}, \texttt{key}, \texttt{content\_type}). The backend uses boto3's SigV4 presigning and returns the URL together with the required headers. Uppy performs the PUT directly to S3/RGW. This keeps the Django process out of the hot path for large transfers, eliminating server memory pressure and Python GIL contention.

\paragraph{CORS (Cross-Origin Resource Sharing).}
Since the frontend origin (e.g.\ \texttt{https://lame-fair.k3s.virtualorfeo.it}interacts with a distinct S3 endpoint, buckets must explicitly allow cross-origin requests. The app configures CORS at bucket creation with:
\begin{itemize}
	\item \textbf{AllowedOrigins}: the single site origin (computed once from env/config).
	\item \textbf{AllowedMethods}: {\texttt{GET, HEAD, PUT, POST, DELETE, OPTIONS}} to cover presigned PUT uploads, presigned GET downloads, listings, and multipart edge cases.
	\item \textbf{AllowedHeaders}: wildcard and common auth/content headers (e.g.\ \texttt{authorization}, \texttt{content-type}, \texttt{x-amz-*}).
	\item \textbf{ExposeHeaders}: \texttt{ETag}, request IDs for debugging.
	\item \textbf{MaxAgeSeconds}: 3600 to reduce preflight churn.
\end{itemize}
A restrictive, origin-pinned configuration prevents third-party origins from reusing presigned URLs via the browser.

\paragraph{Retries and resiliency.}
The S3 client config enables standard retry mode with limited attempts; \texttt{ensure\_bucket} tolerates 404/NoSuchBucket and re-applies CORS, ignoring RGW quirks (e.g.\ \texttt{MalformedXML} noise) where safe. Listings use paginators; downloads stream via \texttt{StreamingBody}. On the HTTP side, large prefix downloads are zipped on the fly with \texttt{zipstream-ng} and \texttt{StreamingHttpResponse} (16,MiB server-to-client buffers) to prevent memory spikes.

\paragraph{Integrity checks.}
For every uploaded key, a background worker computes a SHA-256 (\texttt{MeasurementFileChecksum} table) by streaming the object in fixed chunks. This provides reproducible integrity markers and supports later audit/comparison across mirrors.

\section{Background tasks}\label{sec:background-tasks}

\paragraph{Queueing with Redis RQ.}
The system uses \textbf{Redis Queue (RQ)} for asynchronous work. Web requests enqueue jobs into named queues (\texttt{default}, optionally \texttt{ofed}) stored in Redis. Dedicated worker processes pull from these queues and execute tasks:

\begin{itemize}
	\item \texttt{compute\_checksum(bucket, key)}: streams the object, computes SHA-256, and \texttt{update\_or\_create}s the digest in DB.
	\item \texttt{make\_nexus\_from\_tiff(...)}: orchestrates mapping + build + publish of the \texttt{.nxs} artefact (see \S\ref{sec:nexus-construction}).
\end{itemize}

Workers are decoupled from request lifetimes, keeping the UI responsive even when jobs are CPU or IO heavy. All task functions are importable from the worker container path (\texttt{experiment\_manager.tasks}), satisfying RQ’s import model.

\paragraph{Failure modes and retries.}
Jobs are enqueued with a \texttt{Retry} policy (\texttt{max=3}, backoff intervals). Typical transient failures include S3 timeouts during large reads/writes or temporary certificate/CA issues. Idempotent DB upserts and deterministic storage keys guarantee that replays do not corrupt state. For NeXus builds, job de-duplication uses a stable \texttt{job\_id} to suppress concurrent processing of the same TIFF.

\paragraph{Observability.}
\texttt{django-rq} is mounted under \texttt{/django-rq/} to inspect queues, running jobs, and failures. Application logs annotate enqueue decisions, job IDs, and completion. Exposed presigned URLs for GET (aria2 manifests) include expirations to make download windows auditable.

\section{API and UI surfaces}\label{sec:api-ui}

\paragraph{REST API (DRF).}
The \texttt{api/} package defines serializers and viewsets for domain entities, exposing programmatic access to Projects, Proposals, Samples, and Experiments. The API router aggregates endpoints under a coherent namespace. Serializers enforce the same validation rules as the domain layer to maintain parity between UI and API clients. Test coverage (\texttt{test\_api.py}) verifies shape and permissions.

\paragraph{HTMX modals and the management board.}
The primary operator surface is a three-pane board (\texttt{ManageProposalsView}):

\begin{itemize}
	\item Left: proposals list (\texttt{ProposalPane}).
	\item Middle: samples for the selected proposal (\texttt{SamplePane}).
	\item Right: experiments for the selected sample and the measurement form (\texttt{ExperimentPane}, \texttt{MeasurementPane}).
\end{itemize}

HTMX swaps fragments into \texttt{<div id="...Pane">} containers. Creation flows are handled by modal forms (\texttt{ProposalCreateModal}, \texttt{SampleCreateModal}, \texttt{ExperimentCreateModal}); on success, the view returns \texttt{204} and emits an \texttt{HX-Trigger} to close the modal and refresh the affected pane token (e.g.\ \texttt{experiment-{sample\_id}}). Invalid POSTs return \texttt{200} with the form re-rendered and normalized non-field errors.

\paragraph{Dashboards and landing.}
Authenticated users are routed to role-aware dashboards: \texttt{admin\_dashboard} for privileged users (group “lamepi”), \texttt{regular\_dashboard} otherwise. Guests see a minimal landing and are redirected on login.

\paragraph{Bucket and file views.}
\texttt{/buckets/} lists proposals with their RAW (or mirrored, \texttt{/ofed/}) buckets. \texttt{/buckets/<bucket>/?prefix=...} renders a hierarchical view by scanning keys and partitioning them into “folders” (prefix segments) and files under the current prefix. For single files, presigned GET links are available; for prefixes, the UI offers:
\begin{itemize}
	\item \textbf{ZIP streaming} of the entire sub-tree (store-only ZIP, no compression to avoid CPU spikes).
	\item \textbf{aria2 manifest} generation (\texttt{.aria2.txt}) containing per-object presigned URLs with per-file output paths for high-throughput CLI downloads.
\end{itemize}
Missing buckets render an actionable state; admins can recreate idempotently.

\paragraph{Static assets and client behaviour.}
Custom CSS/JS is minimal. Uppy is bundled for uploads; a small helper initializes the uploader with drag-and-drop and hooks the \texttt{/s3\_presign} endpoint. Modal lifecycle is handled by \texttt{htmx-modals.js} (open/close/esc).

\section{Security model in-app}\label{sec:security}

\paragraph{Authentication and claims.}
Authentication and single sign-on are delegated to Authentik (OIDC). The app never handles passwords. On login, the user is redirected to the IdP; after authorization, an ID token (JWT) is validated and a Django session is established. OIDC endpoints used in logout include an ID token hint and a post-logout redirect.

\paragraph{Claim-to-role mapping and authorization.}
Authorization is enforced via:
\begin{itemize}
	\item \texttt{@login\_required} on all sensitive views.
	\item \texttt{@user\_passes\_test(is\_boss)} for admin-only actions; \texttt{is\_boss} checks group membership (synced from FreeIPA/Authentik).
\end{itemize}
Write operations are further scoped: e.g.\ bucket recreation, mirrored views, and management endpoints are limited to admins. Proposal \texttt{created\_by} persists provenance.

\paragraph{Data plane protections.}
Presigned URLs are scoped to a single object, method, and expiry; browsers are permitted to use them only from the configured origin (CORS). Sensitive configuration (S3 keys, OIDC secrets, CA bundles) is injected via Kubernetes Secrets and not embedded into images. TLS verification pins to the internal CA bundle (FreeIPA) through environment variables consumed by boto3/requests.

\paragraph{Audit trail.}
\texttt{MeasurementFileChecksum} provides immutable fingerprints for uploaded artefacts. README files (per entity and per measurement) document the who/what/when of data creation. Application logs annotate all background job scheduling/execution, including job IDs and storage destinations.

\section{Performance and scalability}\label{sec:performance}

\paragraph{Separation of upload and conversion planes.}
Direct-to-S3 uploads eliminate the server as a bottleneck for large data. The only synchronous server path during uploads is the lightweight \texttt{/s3\_presign} call and the subsequent metadata/registration POST (small payload). Conversion to NeXus, checksumming, and mirroring are fully decoupled in Redis workers, allowing elastic scaling of background throughput independently of web capacity.

\paragraph{Concurrency characteristics.}
\begin{itemize}
	\item \textbf{Uploads:} Uppy can open multiple PUTs concurrently; S3/RGW horizontally scales IO. Backend CPU and memory remain stable since file bytes never traverse Django.
	\item \textbf{Workers:} Spin up 
	N
	N RQ workers (per queue) to parallelize checksum and builder jobs. The deduped \texttt{job\_id} strategy prevents duplicate compute for the same key.
	\item \textbf{Downloads:} Presigned GETs shift bandwidth to the object store. ZIP streaming uses back-pressure through \texttt{StreamingHttpResponse} to avoid buffering the entire archive.
\end{itemize}

\paragraph{Database performance.}
The ORM constraints already imply indexes for uniqueness (e.g.\ (\texttt{project}, \texttt{number}), (\texttt{proposal}, \texttt{slug}), (\texttt{sample}, \texttt{slug})). Additional pragmatic indices can be added if access patterns warrant (e.g.\ on \texttt{MeasurementFileChecksum(bucket, key)}, which is already unique). Selects in list views use \texttt{select\_related} to reduce join chatter.

\paragraph{Caching opportunities.}
\begin{itemize}
	\item \textbf{UI fragments:} Memoize proposal/sample lists for short TTLs to avoid re-query on fast HTMX interactions.
	\item \textbf{Presign results:} Avoid caching (short expiries, key specificity), but consider client-side reuse during multi-file sessions.
	\item \textbf{Readme parsing:} The modal that fetches and parses README could cache parsed lines per object ETag.
\end{itemize}

\paragraph{Network and TLS.}
Presign and S3 clients honor an injected CA bundle; \texttt{AWS\_CA\_BUNDLE}/\texttt{REQUESTS\_CA\_BUNDLE} guard against self-signed errors. Keeping the CA in a mounted Secret avoids baking it into the image. MetalLB + Ingress terminate TLS for app endpoints; presigned URLs point at RGW/MinIO with HTTPS, enabling end-to-end encryption.

\paragraph{Fault tolerance.}
S3 client retries are enabled; background jobs apply backoff and are idempotent. Bucket ensurement tolerates absent buckets and reapplies CORS. The \texttt{/buckets/} UI degrades gracefully on missing buckets (actionable 404 with recreate option for admins).

\paragraph{Operational scaling.}
Web and worker pods can scale independently. Storage is the dominant scaling axis; moving from local-path PVs to a CSI-backed block store (or external S3) is transparent to the code (gateway abstraction). Redis can be swapped for a managed service; RQ workers remain unchanged. The API and UI layers are stateless aside from the DB, facilitating horizontal web scaling behind the Ingress.

\bigskip

\noindent\textbf{Summary.}
The application enforces a strict separation of concerns: domain commands own business invariants; the storage gateway encapsulates S3 details and CORS; Redis RQ absorbs long-running and failure-prone work; and UI/API layers remain thin, reactive façades. Presigned transfers, per-proposal buckets, and mirrored namespaces suit HPC-like, high-throughput environments. The result is a system whose correctness derives from enforced invariants and idempotent writes, whose security leverages OIDC claims and origin-pinned CORS, and whose performance scales with the object store and worker pools rather than with the web tier.